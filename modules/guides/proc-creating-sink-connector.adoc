[id='proc-creating-sink-connector_{context}']
= Creating a {connectors} instance for a data sink
:imagesdir: ../_images

[role="_abstract"]
A _sink_ connector consumes messages from a Kafka topic and sends them to an external system.

For this example, you use the *HTTP Sink* connector which consumes the Kafka messages (produced by your Data Generator source {connectors} instance) and sends the messages to an HTTP endpoint.

.Prerequisites

ifndef::qs[]
* You're logged in to the {product-long-connectors} web console at {service-url-connectors}[^].
endif::[]
* You created a Data Generator source {connectors} instance.
* For the data sink example, open the free https://webhook.site[webhook.site^] in a browser window. The `webhook.site` page provides a unique URL that you copy for use as an HTTP data sink.
* If you want to use a dead letter queue (DLQ) to handle any messaging errors, create a Kafka topic for the DLQ.

.Procedure

. In the {product-long-connectors} web console, click *Create {connectors} instance*.

. Select the sink connector that you want to use:
.. For example, type `http` in the search field. The list of {connectors} is filtered to show the *HTTP sink* connector.
.. Click the *HTTP sink* card and then click *Next*.

. On the *Kafka Instance* page, select the {product-kafka} instance for the connector to work with. For example, select *test-connect*.
+
Click *Next*.

. On the *Deployment* page, the namespace that you select depends on your {openshift} environment.
+
If you're using your own {openshift} environment, select the card for the namespace that was created when a cluster administrator added the {connectors} service to your cluster.
+
If you're using the hosted preview environment, click the *preview namespace* that you provisioned when you created the source connector.
+
Click *Next*.

. Provide the core configuration for your connector:
.. Type a unique name for the connector. For example, type `hello world receiver`.
.. In the *Client ID* and *Client Secret* fields, type the credentials for the service account that you created for {connectors} and then click *Next*.

. Provide the connector-specific configuration for your HTTP sink {connectors} instance:
.. *Topic Names*: Type the name of the topic that you used for the source {connectors} instance. For example, type `test-topic`.
.. *Method*: Accept the default, `POST`.
.. *URL*: Type your unique URL from the link:https://webhook.site[webhook.site^].
.. *Data Shape Consumes Format*: Accept the default, `application/octet-stream`.
+
Click *Next*.

. Select an error handling policy for your {connectors} instance. For example, select *Stop*.
+
Click *Next*.

. Review the summary of the configuration properties and then click *Create {connectors} instance*.
+
Your {connectors} instance is added to the *{connectors} Instances* page.
+
After a couple of seconds, the status of your {connectors} instance changes to the *Ready* state. It consumes messages from the associated Kafka topic and sends them to the data sink (for this example, the data sink is the HTTP URL that you provided).

.Verification
ifdef::qs[]
* Open a web browser tab to your custom URL for the link:https://webhook.site[webhook.site^]. Do you see HTTP POST calls with `"Hello World!!"` messages?

endif::[]

ifndef::qs[]
* Verify that you see HTTP POST calls with `"Hello World!!"` messages. Open a web browser tab to your custom URL for the link:https://webhook.site[webhook.site^].
endif::[]



ifdef::qs[]
[#conclusion]
====
Congratulations! You successfully completed the {product-long-connectors} Getting Started quick start.
====
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
